---
layout: post
title: "Work on Dialogues"
date: 2022-02-24
categories: [NLP]
tags: [nlp, dialogue]
math: true
---

This article is being updated.

Also see post on dialogue summarization.

## Encoder-based Dialogue LM

**Pretraining Methods for Dialog Context Representation Learning**. Mehri et al. ACL'19\
Evaluate four pretraining objectives on dialogue data to improve the dialogue representation.\
<https://www.aclweb.org/anthology/P19-1373/>

**ConveRT: Efficient and Accurate Conversational Representations from Transformers**. Henderson et al. 2019\
Conversational Representations from Transformers: use response-selection as unsupervised pretraining objective for
dialogue tasks. Transformer layers are shared for input and response encoding.\
To encode more history context which is shown to be helpful in other tasks, optionally use another Transformer encoder
to encode history and average two to get final representation (is averaging really useful?).\
<https://arxiv.org/pdf/1911.03688>

**TOD-BERT: Pre-trained Natural Language Understanding for Task-Oriented Dialogue**. Wu et al. EMNLP'20\
Similar to ConveRT: dual encoder for context and response with contrastive loss on response selection.\
<https://www.aclweb.org/anthology/2020.emnlp-main.66>

**Probing Task-Oriented Dialogue Representation from Language Models**. Wu and Xiong. EMNLP'20\
Probe LMs on dialogue datasets by two metrics: (1) linear classifier on example's CLS (2) mutual information on
examples' CLS clustering.\
Finding: ConveRT and TOD-BERT-jnt perform the best.\
Question: freeze CLS or not??\
<https://www.aclweb.org/anthology/2020.emnlp-main.409>

**CONVFIT: Conversational Fine-Tuning of Pretrained Language Models**. Vulic et al. EMNLP'21\
Adapt LM for conversational-based. (1) unsupervised response-ranking loss (2) intent as sentence-similarity loss.\
<https://aclanthology.org/2021.emnlp-main.88>

## Generation-based Dialogue LM

**DIALOGLM: Pre-trained Model for Long Dialogue Understanding and Summarization**. Zhong et al. AAAI'22\
Similar to BART or UniLM on dialogue domain for generation tasks, and introduce dialogue-specific corruption: disrupt content and order of speakers and utterances.\
Sparse attention for long sequence helps little.\
<https://arxiv.org/pdf/2109.02492>

[PLATO](https://mp.weixin.qq.com/s/PmJpnUTqOeGD3-VqK5mdMA)
* PLATO-3: single auto-regressive loss

[SPACE](https://mp.weixin.qq.com/s/NtZH5cibWEqXOWOXzRhYow)
* SPACE-1: fuse dialogue-act pretraining; new DA corpus
* SPACE-2: structured (domain, intent, slot, value) pretraining
* SPACE-3: three decoders: understanding, DA policy, generation

## User Attribute

DialogueNLI: persona sentence <=> relation triple, small portion of utterances <=> persona sentence

**Getting To Know You: User Attribute Extraction from Dialogues**. Wu et al. LREC'20\
Input: utterance; output: relation triple; ALLOW non-exact inference.\
Approach: 2-task pipeline (predicate trigger classification + entity generated by autoregressive)\
Dataset: DialogueNLI on Persona-Chat (use trained NLI model to pair more utterances with persona sentences, so that each utterance now has a paired relation triple\
<https://aclanthology.org/2020.lrec-1.73>

**Extracting and Inferring Personal Attributes from Dialogue**. Wang et al. nlp4convai'22\
Input: utterance; output: relation triple; ALLOW non-exact entities.\
Approach: S2S\
Dataset: DialogueNLI that provides utterances paired with one triple\
<https://aclanthology.org/2022.nlp4convai-1.6>

## Persona

**Stylistic Response Generation by Controlling Personality Traits and Intent**. Saha et al. nlp4convai'22\
<https://aclanthology.org/2022.nlp4convai-1.16>

## Dialogue Downstream Tasks (Non-Generation)

**Dialogue-Based Relation Extraction**. Yu et al. ACL'20\
Propose cross-sentence relation extraction task on dialogue with new dataset and baselines.\
Input: context + two arguments.\
<https://www.aclweb.org/anthology/2020.acl-main.444>

**MIE: A Medical Information Extractor towards Medical Dialogues**. Zhang et al. ACL'20\
Extract (symptom, status) on medical dialogues, with predefined symptoms and status. Treat it as multiclass classification (each symptom-status being a class).\
Instead of naive dual input like QA on BERT, it uses attention to obtain symptom-specific and status-specific representation first on each utterance respectively, then aggregate two representation by cross-utterance attention to get (symptom-specific)-specific utterance representation, then get the final score of each (symptom, status).\
<https://www.aclweb.org/anthology/2020.acl-main.576>

**Learning to Identify Follow-Up Questions in Conversational Question Answering**. Kundu et al. ACL'20\
New task, multiclass classification with triplet input: (candidate follow-up question, passage, conversation history). Model: propose three-way attentive pooling among input.\
<https://www.aclweb.org/anthology/2020.acl-main.90>

**MuTual: A Dataset for Multi-Turn Dialogue Reasoning**. Cui et al. ACL'20\
New dataset on dialogue reasoning with multi-choice question. Input: context + 4 candidates. Baseline: treat it as multiclass classification with dual input.\
<https://www.aclweb.org/anthology/2020.acl-main.130>

**Span-ConveRT: Few-shot Span Extraction for Dialog with Pretrained Conversational Representations**. Coope et al. ACL'20\
Tagging as span extraction. Similar to BERT-LSTM-CRF, this work uses ConveRT-CNN-CRF with some feature tweaking.\
<https://www.aclweb.org/anthology/2020.acl-main.11>

**DialogueGCN: A Graph Convolutional Neural Network for Emotion Recognition in Conversation**. Ghosal et al. EMNLP'19\
Classify emotion for each utterance by modeling speaker-aware dialogue context. Each utterance is the concatenation of two representation: (1) speaker-agnostic representation by sequential encoding on RNN (2) speaker-dependent representation by building utterance graph with speaker dependency and temporal dependency, aggregate utterance/node by GCN.\
<https://www.aclweb.org/anthology/D19-1015>

**Contrast and Generation Make BART a Good Dialogue Emotion Recognizer**. Li et al. AAAI'22\
<https://arxiv.org/abs/2112.11202>

**Enhancing Dialogue Symptom Diagnosis with Global Attention and Symptom Graph**. Chen et al. EMNLP'19\
Task: identify symptom and status in medical conversation. Model: (1) extract symptom by tagging, modeling the utterance with attention on document-level and corpus-level (2) build symptom graph where edge indicates two symptoms co-occur at the same dialogue (3) status classification on each symptom/node.\
<https://www.aclweb.org/anthology/D19-1508>

**Response Selection for Multi-Party Conversations with Dynamic Topic Tracking**. Wang et al. EMNLP'20\
Response selection with auxiliary multi-tasking and fine-grained representation.\
<https://www.aclweb.org/anthology/2020.emnlp-main.533/>
