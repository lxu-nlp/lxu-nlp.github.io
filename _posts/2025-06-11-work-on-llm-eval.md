---
layout: post
title: "Works on LLM Evaluation"
date: 2025-06-11
categories: [NLP]
tags: [nlp, LLM]
math: true
---

See separate post for QA-related evaluation.

**A User-Centric Multi-Intent Benchmark for Evaluating Large Language Models**. Wang et al. EMNLP'24\
Assistant questions.\
<https://aclanthology.org/2024.emnlp-main.210>

**LiveBench: A Challenging, Contamination-Limited LLM Benchmark**. White et al. ICLR'25\
<https://openreview.net/forum?id=sKYHBTAxVa>

**GPQA: A Graduate-Level Google-Proof Q&A Benchmark**. Rein et al. COLM'24\
<https://openreview.net/forum?id=Ti67584b98>


# Vision

**Embodied Agent Interface: Benchmarking LLMs for Embodied Decision Making**. Li et al. NIPS'24\
<https://arxiv.org/abs/2410.07166>

**EmbodiedBench: Comprehensive Benchmarking Multi-modal Large Language Models for Vision-Driven Embodied Agents**. Yang et al. ICML'25\
<https://arxiv.org/abs/2502.09560>
