---
layout: post
title: "Works on QA / Comprehension"
date: 2023-10-20
categories: [NLP]
tags: [nlp, qa]
math: true
---

## QA Dataset: Retrieval (Shallow Pattern)

**WIKIQA: A Challenge Dataset for Open-Domain Question Answering**. Yang et al. EMNLP'15\
(question, snippet candidates), with unanswerable questions.\
<https://aclanthology.org/D15-1237/>

## QA Dataset: Global Context (Deeper Reasoning)

**The NarrativeQA Reading Comprehension Challenge**. Kočiský et al. TACL'18\
Questions on long stories that require higher-level abstraction than superficial pattern matching: majority answers do not have spans directly in text.\
Approach: retrieval & concatenation to shorter doc -> QA.\
<https://aclanthology.org/Q18-1023>

**Fantastic Questions and Where to Find Them: FairytaleQA – An Authentic Dataset for Narrative Comprehension**. Xu et al. ACL'22\
FairytaleQA: similar to NarrativeQA with more diverse questions.\
<https://aclanthology.org/2022.acl-long.34>

**QuALITY: Question Answering with Long Input Texts, Yes!**. Pang et al. NAACL'22\
More abstractive questions: how, why, description.\
Multi-choice.\
<https://aclanthology.org/2022.naacl-main.391>

## QA Dataset: Question Decomposition

**Did Aristotle Use a Laptop? A Question Answering Benchmark with Implicit Reasoning Strategies**. Geva et al. TACL'21\
STRATEGYQA: Boolean QA benchmark with implicit multi-hop reasoning/decomposition (implicit: reasoning has little overlap with questions).\
<https://aclanthology.org/2021.tacl-1.21/>

**When Do Decompositions Help for Machine Reading?**. Wei et al. EMNLP'23\
<https://aclanthology.org/2023.emnlp-main.219>

**Small Language Models Fine-tuned to Coordinate Larger Language Models improve Complex Reasoning**. Juneja et al. EMNLP'23\
Train separate decomposer with RL.\
<https://aclanthology.org/2023.emnlp-main.225>

## Approach: Multi-hop on Structure

**Think-on-Graph: Deep and Responsible Reasoning of Large Language Model on Knowledge Graph**. Sun et al. 2023\
Reasoning hop as a beam search process, where LLM performs selection on R1 E1 R2 E2 ...\
LLM recognizes relevance much better than embedding/lexical similarity.\
<https://arxiv.org/abs/2307.07697>
