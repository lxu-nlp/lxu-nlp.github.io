---
layout: post
title: "AI (2): Overview of Hidden Markov Model (HMM) and Viterbi Algorithm"
date: 2018-12-01
categories: [Machine Learning]
tags: [ml, hmm]
math: true
---

This article is part of my review of the classic Artificial Intelligence (AI) course. In this post, we will examine the basic things related with the Hidden Markov Model ([HMM](https://en.wikipedia.org/wiki/Hidden_Markov_model)) and its inference, including [Viterbi Algorithm](https://en.wikipedia.org/wiki/Viterbi_algorithm). Figures are from the slides by Prof. Eugene Agichtein.

## Bayesian Network

[Bayesian Network](https://en.wikipedia.org/wiki/Bayesian_network), or Bayes Network, is a general model for statistical inference that is based on Bayes' Theorem. It defines a directed acyclic graph (DAG) that represents the conditional dependencies of each node, where each node can be any variables of statistical interests, such as events or latent variables.

The inference on Bayes Net is based on the basic probabilistic inference concepts, such as conditional probabilities, conditional independence, joint probabilities. For an arbitrary graph, the inference is NP-hard. Here, we only focus on a special and simple case of Bayes Net: chain (linear) structure.

## Markov Models

A Markov Model (or [Markov Chain](https://en.wikipedia.org/wiki/Markov_chain)) is a chain-structured Bayes Net. It assumes the Markov Property, where the next future state completely depends on the current state ("memoryless").

![hmm](/assets/img/legacy/hmm1.png)

The sequence probability (joint probability):

$$
\begin{align*}
P(x_1, x_2, x_3, \dots, x_n) &= P(x_1) P(x_2 | x_1) P(x_3 | x_1, x_2) \dots P(x_n | x_1, \dots, x_{n-1}) \\
&= P(x_1) P(x_2 | x_1) P(x_3 | x_2) \dots P(x_n | x_{n-1}) \quad\quad\text{(Markov Property)}\\
&= P(x_1) \prod_{i=2}^n P(x_i | x_{i-1})
\end{align*}
$$

Denote $U(n)= P(x_1, \dots, x_n)$, we can also write the sequence probability as a recursion form:

$$U(n) = U(n-1) P(x_n | x_{n-1})$$

## Hidden Markov Models (HMMs)

HMM is when we differentiate hidden/latent states (our belief states) and the observed states (emissions) on the Markov Model. It can be seen that each hidden state emits an observation. A common scenario is the sequence tagging problem, such as Part-of-Speech (POS) tagging, Named Entity Recognition (NER).

![hmm](/assets/img/legacy/hmm2.png)

Take POS tagging as an example: $x_i$ is the hidden state, which is the POS tag we want to know; $e_i$ is the observation, which is the actual words we can observe. We can use HMM inference to get the most likely sequence tags based on the sequence of observation.

To accommodate both hidden states and emission states, we need to differentiate two types of probabilities:
* Transition probability (the right arrow): $$P(x_i | x_{i-1})$$
* Emission probability (the down arrow): $$P(e_i | x_i)$$

To model both hidden states and emission states in HMM, the sequence probability is now changed to:

$$
\begin{align*}
P(x_1, e_1, x_2, e_2, \dots, x_n, e_n) &= P(x_1) P(e_1 | x_1) P(x_2 | x_1) P(e_2| x_2) \dots P(x_n | x_{n-1}) P(e_n | x_n)\\
&= P(x_1)  P(e_1 | x_1) \prod_{i=2}^n P(x_i | x_{i-1}) P(e_i | x_i)
\end{align*}
$$

Denote $U(n) = P(x_1, e_1, \dots, x_n, e_n)$, we can write the sequence probability in the recursion form:

$$
\begin{align*}
U(n) = \underbrace{U(n-1) P(x_n | x_{n-1})}_{\text{Prior}_n} \cdot \underbrace{P(e_n | x_n)}_{\text{Likelihood}_n}
\end{align*}
$$

In other words, we can regard each step as a basic building block of Bayesâ€™ Theorem: $U(n)$ is the posterior sequence probability till step $i$; it is proportional to the prior sequence probability $U(n-1) P(x_n | x_{n-1})$, normalized by the likelihood $P(e_n | x_n)$ which is the emission probability.

Note that above calculates the joint sequence probability. We might be also interested in the probability of an individual hidden state at a specific step. We will answer both questions in the following.

## Inference/Decoding

Given the transition probability and emission probability, we can ask the following two questions.

- Q1: what is the most probable hidden state as step $i$ for a sequence of observation? (conditional probability)
- Q2: what is the most probable entire hidden state sequence for a sequence of observation? (joint probability)

These are two different perspectives to look at the hidden states, and it depends on the use case. For the tagging problem, we are usually more interested in the entire sequence of hidden states (Q2). For other problems such as weather forecasting, we are more interested in each individual hidden state at a specific step (Q1).

For each question respectively, we call the inference/decoding algorithm as follows:

- Q1: Forward-Backward (for individual hidden states)
- Q2: Viterbi Algorithm (for sequence of hidden states)

Both two algorithms are similar and based on dynamic programming (enabled by Markov property), following the previous recursion form. They can be seen as operating on a $n \times |C|$ grid, where the joint or marginal probability of hidden states at a specific step is calculated, and the recursion process keeps moving forward.

![hmm](/assets/img/legacy/hmm3.png)

##### Forward Algorithm

Denote $V(x_n)$ as the probability of being a specific hidden state $x_n$ at step $n$. At each step, we want to take the marginal probability of each hidden state, as the prior:

$$V(x_n) = \overbrace{P(e_n | x_n)}^{\text{Likelihood}_n} \cdot \overbrace{\sum_{x'_{n-1} \in X} V(x'_{n-1}) P(x_n | x'_{n-1})}^{\text{Prior}_n}$$

For Q1, the most probable hidden state at step $i$ is $\text{argmax}_{x_i} V(x_i)$, which is simply the hidden state with the highest probability. Here we only illustrate the forward algorithm to calculate $V(x_n)$; an example would be:

![hmm](/assets/img/legacy/hmm4.png)

##### Viterbi Decoding

For Q2, we want to choose one hidden state at each step to form the most probable sequence of hidden states. Thus, at each step, we are only interested in the sequence joint probability. Denote $V(x_n)$ as the highest sequence probability that ends with $x_n$ at step $n$. At each step, instead of taking the marginal probability for each hidden state, we only take the maximum probability:

$$V(x_n) = \overbrace{P(e_n | x_n)}^{\text{Likelihood}_n} \cdot \overbrace{\max_{x'_{n-1} \in X} V(x'_{n-1}) P(x_n | x'_{n-1})}^{\text{Prior}_n}$$

In other words, given the previous best sequence, the next best sequence is obtained by simply searching over the hidden state space and taking the maximum.The probability of the final best sequence is simply $\text{argmax}_{x_n} V(x_n)$; the sequence itself can be obtained from back-tracing.
