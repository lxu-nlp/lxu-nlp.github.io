---
layout: post
title: "Machine Learning (9) SVM: Overview of SVM Optimization, Lagrange Duality and KKT Conditions"
date: 2018-11-02
categories: [Machine Learning]
tags: [ml, svm]
math: true
---

This article is part of my review of Machine Learning course. It introduces Support Vector Machine (**SVM**) classifier, the form of its corresponding convex optimization, and how to use Lagrange Duality and KKT Conditions to solve the optimization problem. Additionally, it also introduces how to make use of Kernel tricks to do non-linear classification.

Some pictures in this article are from Dr. Joyce Ho's course slides.

## Support Vector Machine (SVM) Overview

Let's consider a classification problem of a linearly-separable case.

![svm1](/assets/img/legacy/svm1.png)

Since there are more than one boundaries to completely separate these two classes, which one is the best solution?

Basically, SVM decides that the best line (or hyperplane in high dimensions) separator is the one with maximum margin (as shown below). Every sample should ideally fall outside the margin. The larger the margin, the more confident we are that an unseen sample will be correctly classified. SVM turns out to work pretty well in both theory and practice.

![svm1](/assets/img/legacy/svm2.png)

The concept of separator and margin is easy to see for linearly-separable case. What if there are noises, such that no hyperplane can completely separate two classes? Or even worse, what if the two classes are not linearly-separable at all (e.g. the boundary is a hypersphere)?

Short answers:
1. The concept we mentioned above is called "hard-margin". But in reality, data are usually not completely linearly-separable. To generalize, SVM allows some slackness around the separator, meaning it allows some misclassified samples, with a hyper-parameter that indicates the penalty strength $C$. The larger the penalty strength, the less misclassified samples, but also the tighter the margin (which is not what we want). Large $C$ will cause overfitting, and the value of $C$ represents the trade-off between misclassification rate and simplicity of the model. The margin with slackness is called "soft-margin". In this article, for simplicity, we are only going to look at the "hard-margin" optimization.
2. For the non-linear boundary case, SVM uses "kernel tricks" to compute higher-dimensional features. Kernel trick is a way to compute the inner product of two high-dimensional vectors efficiently, without visiting the high dimensions.  Later in this article, we will see why we can use kernels in SVM.

In summary, these are two key ideas of SVM:
* Fit the separator with the largest margin to make it robust
* Use kernel tricks to efficiently compute larger feature space

## Optimization Problem (Hard-Margin Case)

##### Basic Components (knowledge review)

1. The target separator to fit: $wx + b = 0$, which is a hyperplane. $w$ and $b$ are parameters that SVM needs to learn from training samples. Note that $w$ is the normal vector of this hyperplane; $wx + b = 0$ essentially means all vectors that are orthogonal to $w$ (with intercept $b$) forms a hyperplane.
2. The distance from a point $x'$ to target hyperplane: $\dfrac{wx' + b}{\Vert w\Vert}$.
   * Proof: select any point $x$ on the hyperplane. The distance from $x'$ to the hyperplane, is equal to the projection of vector $x' - x$ on the normal vector $w$. Therefore, distance $ = \Vert x' - x\Vert \cdot \cos (x'-x, w) \\=\Vert x' - x\Vert \cdot \dfrac{w(x'-x)}{\Vert w\Vert\Vert x' - x\Vert} \\= \dfrac{wx' - wx}{\Vert w\Vert} =\dfrac{wx' + b}{\Vert w\Vert}$
3. Assume labels are $+1, -1$. The margin $\gamma_i$ formed by sample $x_i$: $\gamma_i = \dfrac{y_i(wx_i + b)}{\Vert w\Vert}$. When correctly classified, $\gamma_i > 0$.

![svm1](/assets/img/legacy/svm3.png)

##### Optimization Problem

The objective is to maximum the margin, where all samples have distance larger than margin (no samples are inside margin area). It can be represented by the following optimization problem:

$$
\max_{w, b} \quad \gamma\\
\text{s.t.} \quad \gamma_i \geq \gamma
$$

Let's multiply $\Vert w\Vert$ on constraints, so that $\Vert w\Vert$ is no longer denominator.

$$
\max_{w, b} \quad \gamma\\
\text{s.t.} \quad y_i(wx_i + b) \geq \gamma \Vert w\Vert
$$

Let $\hat{\gamma} = \gamma \Vert w\Vert$. Therefore $\gamma =\hat{\gamma} / \Vert w\Vert$.

$$
\max_{w, b} \quad \dfrac{\hat{\gamma}}{\Vert w\Vert}\\
\text{s.t.} \quad y_i(wx_i + b) \geq \hat{\gamma}
$$

Observe that unlike $\gamma$, the value of $\hat{\gamma}$ actually doesn't matter. We can give a scale $\lambda$ to $w$ and $b$ which scales $\hat{\gamma}$ to $\lambda\hat{\gamma}$, and the target is the same: $\dfrac{\lambda \hat{\gamma}}{\lambda \Vert w\Vert} =\dfrac{\hat{\gamma}}{\Vert w\Vert}$. Therefore, different values of $\hat{\gamma}$ doesn't change the optimization. we can just set $\hat{\gamma} = 1$. Now the optimization problem is:

$$
\max_{w, b} \quad \dfrac{1}{\Vert w\Vert}\\
\text{s.t.} \quad y_i(wx_i + b) \geq 1
$$

Let's inverse the target and use squared norm (squared norm is easier for later). Now we have a minimization problem:

$$
\min_{w, b} \quad \dfrac{\Vert w\Vert^2}{2}\\
\text{s.t.} \quad y_i(wx_i + b) \geq 1
$$

## Lagrange Duality and KKT Conditions

Let's put aside the SVM optimization temporarily, and review the general approaches to solve optimization problems with constraints.

##### Optimization with equality constraints

We can just construct Lagrangian $\mathcal{L}(w, \beta)$, and set partial derivative of $w$ and $\beta$ to $0$ to solve for $w$ and $\beta$. It is expressed as below.

Optimization problem:

$$
\min_w \quad f(w)\\
\text{s.t.} \quad h_i(w) = 0
$$

Lagrangian:

$$
\mathcal{L}(w, \beta) = f(w) + \sum \beta_ih_i(w)
$$

##### Optimization with inequality constraints

We call the problem as Primal Optimization Problem, and to solve it, we first construct a generalized Lagrangian.

Optimization problem:

$$
\min_w \quad f(w)\\
\begin{align}
\text{s.t.} \quad h_i(w) = 0\\
g_i(w) \leq 0
\end{align}
$$

Generalized Lagrangian:

$$
\mathcal{L}(w, \alpha, \beta) = f(w) + \sum \alpha_i g_i(w) + \sum \beta_i h_i(w)
$$

Now our problem is to minimize $\mathcal{L}(w, \alpha, \beta)$. Let's rewrite the primal problem to another form. Consider the following problem where $w$ can violate the constraints:

$$
\max_{\alpha, \beta: \alpha_i \geq 0} \mathcal{L}(w, \alpha, \beta)
$$

Notice that if $w$ satisfies all constraints of $g_i(w) \leq 0$ and $h_i(w) = 0$, then the maximum of Lagrangian is $f(w)$, when $\alpha = 0$. If $w$ violates any i-th constraint, the maximum of Lagrangian is $+\infty$, because we can set $\alpha_i$ or $\beta_i$ so that $\alpha_i g_i(w)$ or $\beta_i h_i(w)$ is $+\infty$. Therefore, $\max_{\alpha, \beta: \alpha_i \geq 0} \mathcal{L}(w, \alpha, \beta)$ takes the same value of original objective $f(w)$ for all values of $w$ that satisfy the constraints. It can be expressed as:

$$
\max_{\alpha, \beta: \alpha_i \geq 0} \mathcal{L}(w, \alpha, \beta) =
\begin{cases}
f(w) \quad w \text{ satisfies all primal constraints}\\
+\infty \quad \text{Otherwise}
\end{cases}
$$

If we then add a minimization, the following new optimization problem is the same as the original primal problem.

$$
\min_w \max_{\alpha, \beta: \alpha_i \geq 0} \mathcal{L}(w, \alpha, \beta)
$$

We can also take a look at its dual problem. The dual problem here is just to change the order of "max" and "min":

$$
\max_{\alpha, \beta: \alpha_i \geq 0} \min_w \mathcal{L}(w, \alpha, \beta)
$$

Let's denote the solution of primal problem as $p^*$, and solution of dual problem as $d^*$. A function's "max min" is always less than or equal to its "min max":

$$
d^* = \max_{\alpha, \beta: \alpha_i \geq 0} \min_w \mathcal{L}(w, \alpha, \beta) \leq \min_w \max_{\alpha, \beta: \alpha_i \geq 0} \mathcal{L}(w, \alpha, \beta) = p^*
$$

Under the following conditions ([KKT Conditions](https://en.wikipedia.org/wiki/Karush%E2%80%93Kuhn%E2%80%93Tucker_conditions)) for all $i$, the solution for primal and dual is the same: $d^* = p^*$. Therefore, if an optimization problem satisfies all KKT Conditions, we can either solve the primal directly (which is often hard), or we can opt to solve the dual problem (which is more common).

$$
\begin{align}
\dfrac{\partial \mathcal{L}}{w_i} &= 0\\
\dfrac{\partial \mathcal{L}}{\beta_i} &= 0\\
\alpha_i g_i(w) &= 0\\
g_i(w) &\leq 0\\
\alpha_i &\geq 0
\end{align}
$$

Observe the above conditions, we can see the first two conditions are the same as Lagrangian conditions. Furthermore, we can see a relation between $a_i$ and $g_i(w)$: if $a_i > 0$, then $g_i(w) = 0$; if $g_i(w) < 0$, then $\alpha_i = 0$. Later we will see what this relation means in the context of SVM optimization.

## Solving SVM Optimization by Solving Dual Problem

Recall that the SVM optimization is as follows:

$$
\min_{w, b} \quad \dfrac{\Vert w\Vert^2}{2}\\
\text{s.t.} \quad g_i(w) = -[y_i(wx_i + b) - 1] \geq 0
$$

Here is the overall idea of solving SVM optimization: for the Lagrangian of SVM optimization (with linear constraints), it satisfies all the KKT Conditions. Therefore, we can solve it by solving its dual problem, and the dual problem has some nice properties that allows us to use Kernel trick.

Let's first get the Lagrangian of the SVM optimization:

$$
\mathcal{L}(w, b, \alpha) = \dfrac{\Vert w\Vert^2}{2} - \sum \alpha_i [y_i (wx_i + b) - 1]
$$

From the KKT Conditions, we know that when  $y_i (wx_i + b) - 1 < 0$, $\alpha_i = 0$; and $\alpha_i$ will be non-zero only when $y_i (wx_i + b) - 1 = 0$. Such samples $(x_i, y_i)$ are called Support Vectors, and they lie on the target margin. The size of Support Vectors are much smaller than the size of training samples. In the below picture, those three samples that lie on the margin are Support Vectors.

##### Primal and Dual Problem

The primal problem is:

$$
\min_{w, b} \max_{\alpha:\alpha_i \geq 0} \mathcal{L}(w, b, \alpha)\\
\min_{w, b} \max_{\alpha:\alpha_i \geq 0} \dfrac{\Vert w\Vert^2}{2} - \sum \alpha_i [y_i (wx_i + b) - 1]
$$

The dual problem is:

$$
\max_{\alpha:\alpha_i \geq 0} \min_{w, b} \mathcal{L}(w, b, \alpha)\\
\max_{\alpha:\alpha_i \geq 0} \min_{w, b} \dfrac{\Vert w\Vert^2}{2} - \sum \alpha_i [y_i (wx_i + b) - 1]
$$

Another way to think about the max

To solve the dual problem, we need to first minimize Lagrangian with respect to $w$ and $b$. Therefore, take partial derivative of $w$ and $b$ and set to $0$, we get two other equations:

Partial derivative of $w$ to $0$:

$$
\begin{align}
\dfrac{\partial \mathcal{L}}{w} &= 0\\
w - \sum \alpha_i y_i x_i &= 0\\
w &= \sum \alpha_i y_i x_i
\end{align}
$$

Partial derivative of $b$ to $0$:

$$
\begin{align}
\dfrac{\partial \mathcal{L}}{b} &= 0\\
\sum \alpha_i y_i &= 0
\end{align}
$$

Plug in these two equations to Lagrangian, we get:

$$
\mathcal{L}(w, b, \alpha) = \sum \alpha_i  - \dfrac{1}{2} \sum_{i, j} y_i y_j \alpha_i \alpha_j x_i x_j
$$

The next task is to solve the following optimization. We call it the dual version of SVM:

$$
\max_{\alpha} \sum \alpha_i  - \dfrac{1}{2} \sum_{i, j} y_i y_j \alpha_i \alpha_j x_i x_j\\
\begin{align}
\text{s.t.} \quad \alpha_i &\geq 0\\
\sum \alpha_i y_i &= 0
\end{align}
$$.

If we can solve the dual version of SVM and get the solution $\alpha^*$, we can just use $w = \sum \alpha_i y_i x_i$ to get the solution $w^*$, and it is not hard to get the solution $b^*$. A common algorithm to solve the dual is [SMO](https://en.wikipedia.org/wiki/Sequential_minimal_optimization) , but it is outside the scope of this article.

By far, we now have the solution $\alpha^*, w^*, b^*$ for the dual problem, and because Lagrangian of SVM satisfies all KKT Conditions, $\alpha^*, w^*, b^*$ is also the solution for our primal problem, therefore, SVM optimization is solved.

## Kernel Tricks

Let's look at the SVM solution we get from the dual problem:

$$
\begin{align}
y = w^* x + b^* &= (\sum \alpha_i^* y_i x_i)x + b^*\\
&= \sum \alpha_i^* y_i <x_i \cdot x> + b^*
\end{align}
$$

From KKT Conditions, we know that $\alpha_i^*$ will be zero for all non-support vectors. Therefore, many terms of $\alpha_i^* y_i <x_i \cdot x>$ will be zero, and we only need to compute the inner product between input vector $x$ and support vectors. This is one of the benefits that dual problem brings to us: efficiency when predicting.

Furthermore, for prediction, the only operation on input $x$ is to compute the inner product between $x$ and support vectors: $<x_i \cdot x>$. Therefore, it allows us to use [Kernel Tricks](https://en.wikipedia.org/wiki/Kernel_method) to increase the feature space, without explicit visit to higher dimensional feature space. This is another powerful benefits brought by the dual form: we don't need any additional heavy computation during training and prediction, and we can increase the feature space for free!

A kernel has the following form:

$$K(x, z) = \Phi(x) \cdot \Phi(z)$$

where $\Phi$ is feature space mapping. The kernel $K(x, z)$ computes the inner product of two feature space mapping, by directly mapping the inner product of two input vectors, which is often inexpensive compared to $\Phi(x) \cdot \Phi(z)$.

One of the popular kernels is [Polynomial Kernel](https://en.wikipedia.org/wiki/Polynomial_kernel), and we can use it to increase the degree of feature space in SVM.
