---
layout: post
title: "Machine Learning (7) Boosting: Gradient Boosting vs. Forward Stagewise Boosting"
date: 2018-10-29
categories: [Machine Learning]
tags: [ml, boosting]
math: true
---

This article is part of my review of Machine Learning course. It introduces another boosting method: **Gradient Boosting**, its generic form and tree form, and briefly discusses its similarities and differences with **Forward Stagewise Boosting**. For a review of the boosting concept and Forward Stagewise Boosting (or Forward Stagewise Additive Modeling), you can refer to my previous post.

Some pictures are from the textbook "The Elements of Statistical Learning" (ESL).

## Generic Gradient Boosting

Let me show the Gradient Boosting algorithm later - let's first take a look of where the idea of Gradient Boosting comes from.

Let $f(x)$ be our boosting predictor to fit. The goal is always to minimize the loss function:

$$minimize \quad L(f) = \sum L(y_i, f(x_i))$$

Which means, the goal is to minimize $L(f)$ with respect to $f$. If we treat it as a numerical optimization problem, we can use a greedy strategy to solve it: during each iteration, we calculate the  derivative of $f_{m-1}$ on the loss function, as the target for current learner $g_m$ to learn, and fits a step length $\gamma_m$ such that the new model $f_{m-1}(x) + \gamma_m g_m(x)$ has min loss:

$$\gamma_m = argmin \; \sum L(y_i, f_{m-1}(x_i) + \gamma g_m(x_i))$$

Overall, each iteration trains an additive model that fits the gradient of $f_{m-1}$, or fits towards to the direction where $f_{m-1}$ is most rapidly decreasing. And we give a step length to this gradient, where this step length produces min loss. If we keep iterating and add new additive models to correct the predictor $f$, the loss  $L(f) = \sum L(y_i, f(x_i))$ should gradually decrease.

Notice that this sounds very familiar - we are optimizing a loss function by iteratively moving towards to the deepest gradient with a step length (learning rate). It has the exact same idea as gradient descent; the gradient boosting process is like a gradient descent process, which is really a numerical optimization process using first-order (or potentially second-order) derivative.

Now let's see the generic form of Gradient Boosting (from [Wiki](https://en.wikipedia.org/wiki/Gradient_boosting#Algorithm)):

![generic-boosting](/assets/img/legacy/gradient-boosting.png)

**Adding Shrinkage**: when we update the model, we can also include a shrinkage strength parameter $\nu$, and the updated model is:

$$F_m(x) = F_{m-1}(x) + \nu \gamma_m h_m(x)$$

**Adding Subsampling** (Stochastic Gradient Boosting): when we compute the gradient, we can also include a subsampling rate, so that the gradient is only calculated on a subset of training samples. The idea is the same as stochastic gradient descent.

## Gradient Tree Boosting

Gradient Boosting usually uses regression trees as base learners. For trees, we can adapt the generic Gradient Boosting algorithm to work better. Because decision tree has the unique feature that, it produces same output for all samples in the same region, therefore, we can assign multiple coefficients to the new tree learner - one coefficient for each region. Assigning multiple coefficients is not possible for models like linear regression.

Let's take a look at Gradient Tree Boosting algorithm (from ESL):

![generic-boosting](/assets/img/legacy/gradient-boosting-tree.png)

Here, $\gamma_{jm}$ is the coefficient or the estimator assigned to each region $R_{jm}$ at the m-th iteration. Assuming the problem is binary classification, Notice that the tree representation is:

$$T(x) = \sum_{j=1}^{J} \gamma_j \mathbb{1} (x \in R_j)$$, where $R_j$ is the j-th region in the tree, and $\gamma_{j}$ is the estimator for region $R_j$.

## Comparison with Forward Stagewise Boosting

Gradient Boosting and Forward Stagewise Boosting (or Forward Stagewise Additive Modeling) are two variants of Boosting, and both linearly combines multiple weighted weak learners together. In both algorithms, coefficients are chosen greedily which aims to minimize the loss function.

However, these two algorithms think about the problem from two perspectives, and have different ways to obtain the new learner:

Gradient Boosting considers the loss as a numerical optimization problem: each new added learner tries to be the gradient of the previous model; each learner with its step length is a separate correction term to its previous model.
Forward Stagewise Boosting considers the loss itself as a whole piece: during each iteration, each added learner and its associated coefficient are selected in a way that, the new updated model (previous + new learner) as a whole has min loss.
