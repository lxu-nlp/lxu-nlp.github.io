---
layout: post
title: "Machine Learning (8): Bagging, Random Forest and Out-of-Bag Samples"
date: 2018-10-29
categories: [Machine Learning]
tags: [ml, bagging]
math: true
---

This article is part of my review of Machine Learning course. It introduces the concept of **Bagging**, and one famous variant of Bagging using decision tree: **Random Forest**. The usage of Out-of-Bag (OOB) samples is also discussed here, specifically, OOB Error and feature importance.

Some pictures are from the textbook "The Elements of Statistical Learning" (ESL).

## Bagging

Bagging is a variance reduction technique, and the main idea is to have a collection of predictors trained on [bootstrap](https://en.wikipedia.org/wiki/Bootstrapping_(statistics)) samples, and the final prediction is the average or majority vote among all predictors.

The general procedure of Bagging can be simply expressed as the following:

1. Create $B$ bootstrap replicates.
2. Fit a predictor for each bootstrap replicate. (Totally $B$ predictors)
3. Combine prediction via averaging or voting.
   * For regression problem, average predictions.
   * For classification problem, average probabilities (preferred) or take majority vote.

### Why Does Bagging Work?

Bagging can reduce variance by using multiple predictors on bootstrap samples, which is good enough; but it also increases the bias. How do we know Bagging would work well? Here is my own understanding and hopefully it can give an intuition.

Suppose for a binary classification problem, we have $K$ independent classifiers $f_k$, and each has the same misclassification rate $e$. The final misclassification rate $\hat{e}$ of the bagged classifier $\hat{f}$ forms a binomial distribution:

$$\hat{e} \sim B(K, e)$$

According to [Central Limit Theorem](https://en.wikipedia.org/wiki/Central_limit_theorem), the binomial distribution can be approximated using normal distribution, when $K \to\infty$:

$$\hat{e} \sim N(Ke, Ke(1-e))$$

Let $K' = \lambda K$, which we increase the scale of $K$ by a factor of $\lambda$. We know that for a certain normal distribution, the variance would become $\lambda^2$ times larger. However, here the variance becomes $\lambda Ke(1-e)$, which is only $\lambda$ times larger. The relative variance actually decreases by a factor of $\dfrac{1}{\lambda}$. Therefore, when $K$ gets larger, the normal distribution is more tightly bounded near the mean value, which means the underlying area (or probability) near the mean value increases.

Since in the distribution, the underlying area around $Ke$ increases while $K$ gets larger, when $e < 0.5$, the underlying area that $> 0.5K$ will gets smaller, which essentially means the probability of misclassified sample will be smaller. When $K \to\infty$ ($\lambda \to \infty$), the relative variance will asymptotically become $0$ ($\dfrac{1}{\lambda} \to 0$), so the probability of misclassified sample will asymptotic become $0$.

On the other side, if $e > 0.5$, when $K \to\infty$ ($\lambda \to \infty$), the probability of misclassified sample will   asymptotically become $1$. This is the case when bagging doesn't work: if each classifier is worse than random guess.

Last thing to note is, the above analysis is only in theory, and we are assuming each classifier is independent, which is not usually the case in reality.

## Random Forest

Random Forest is based on Bagging method using decision trees as classifiers, but takes one step further to reduce more variance. Before we get into the algorithm, let's take a look at the motivation.

For $B$ identically distributed variables with pairwise correlation $p$, the variance of the average is:

$$p \sigma + \dfrac{1-p}{B} \sigma^2$$

The first term is not a function of $B$. As $B$ increases, the first term will remain the same, which limits the benefits of Bagging. The idea of Random Forest is to further reduce the correlation $p$ between each tree, and this is done by only considering a subset of features when splitting tree nodes.

Here is the algorithm:

![random-forest](/assets/img/legacy/random_forest.png)

In summary, Random Forest is just a bagged classifier using trees, and at each split, only considers a subset of features randomly to reduce tree correlation. Random Forest works very well in general, and is a good off-the-shelf predictor.

## Out-of-Bag (OOB) Samples

OOB samples are those samples that are not included in the bootstrap samples. OOB samples are unique to Bagging, or any bootstrap-aggregated methods.

### OOB Error

One important usage of OOB samples is OOB Error, which serves as validation error, without the need to set aside a validation set. Here is how to get OOB Error in Random Forest:

![random-forest](/assets/img/legacy/oob.png)

Simply said, OOB samples are essentially the validation set; they are generated by bootstrap process, so we don't need to do the explicit setup.

An important observation is that, OOB Error can be computed along the way we train the Random Forest.

### Feature Importance

Another usage of OOB samples is to compute the feature importance. From the textbook ESL:

"When the bth tree is grown, the OOB samples are passed down the tree, and the prediction accuracy is recorded. Then the values for the jth variable are randomly permuted in the oob samples, and the accuracy is again computed. The decrease in accuracy as a result of this permuting is averaged over all trees, and is used as a measure of the importance of variable j in the random forest."

Simply said, for OOB samples of each tree, we do a permutation for each feature, and the final feature importance is the averaged decrement of accuracy after permutation.

### A Simple RF Implementation with OOB Error and Feature Importance

Here is a simple Random Forest implementation (for binary classification with label $0, 1$) that I wrote for my homework. Besides normal training and predicting methods, it computes the OOB Error and feature importance along with the training process.

The evaluation metric is AUC under ROC.

* "nest" is the number of trees in the forest
* "q" is the ratio of number of features to consider at each split
* "max_depth" and "min_leaf" are tree parameters

```python
# Define class of random forest
from sklearn.tree import DecisionTreeClassifier
from sklearn.utils import resample
from sklearn.metrics import roc_auc_score

class RandomForestClassifier:
    def __init__(self, nest=25, max_depth=10, min_leaf=0.05, q=0.8):
        self.nest = nest
        self.max_depth = max_depth
        self.min_leaf = min_leaf
        self.q = q
        self.trees = None
    
    # Define training method, return OOB AUC and feature importance
    def train(self, X, y, nest=None, max_depth=None, min_leaf=None, q=None):
        self.trees = []
        oob_prob = [[] for i in range(y.size)]
        feature_importance = [[] for i in range(X.shape[1])]
        
        self.nest = nest if nest is not None else self.nest
        self.max_depth = max_depth if max_depth is not None else self.max_depth
        self.min_leaf = min_leaf if min_leaf is not None else self.min_leaf
        self.q = q if q is not None else self.q
        
        for b in range(self.nest):   
            idx_spl = resample(np.arange(y.shape[0]))
            X_spl = X[idx_spl]
            y_spl = y[idx_spl]
            
            dtc = DecisionTreeClassifier(max_depth=self.max_depth,
                                         min_samples_leaf=self.min_leaf,
                                         max_features=self.q)
            dtc.fit(X_spl, y_spl)
            self.trees.append(dtc)
            
            # Record OOB sample prediction (probability of positive label)
            idx_oob = np.setdiff1d(np.arange(y.shape[0]), idx_spl)
            X_oob = X[idx_oob]
            y_oob = y[idx_oob]
            y_oob_prob = dtc.predict_proba(X_oob)[:, 1]
            for i in range(idx_oob.size):
                oob_prob[idx_oob[i]].append(y_oob_prob[i])
            
            # Record feature importance
            accuracy = roc_auc_score(y_oob, y_oob_prob)
            for i in range(X_oob.shape[1]):
                X_oob_permuted = X_oob.copy()
                X_oob_permuted[:, i] = np.random.permutation(X_oob[:, i])
                accuracy_permuted = roc_auc_score(y_oob, dtc.predict_proba(X_oob_permuted)[:, 1])
                feature_importance[i].append(accuracy - accuracy_permuted)
        
        # Compute final OOB accuracy
        oob_prob_mean = []
        idx_ever_oob = []
        for i in range(y.size):
            if len(oob_prob[i]) != 0:
                idx_ever_oob.append(i)
                oob_prob_mean.append(np.array(oob_prob[i]).mean())
        
        accuracy_oob = roc_auc_score(y[idx_ever_oob], oob_prob_mean)
        
        # Compute final feature importance
        feature_importance_mean = np.array(feature_importance).mean(axis=1)
        
        return accuracy_oob, feature_importance_mean
    
    # Define prediction method: return predicted label, and probability of positive label
    def predict(self, X):
        pos_prob = []
        for i in range(len(self.trees)):
            pos_prob.append(self.trees[i].predict_proba(X)[:, 1])
        pos_prob_mean = np.array(pos_prob).mean(axis=0)
        return 1 * (pos_prob_mean > 0.5), pos_prob_mean
```
