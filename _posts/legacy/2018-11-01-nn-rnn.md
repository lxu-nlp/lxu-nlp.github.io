---
layout: post
title: "Neural Network (2): RNN and Problems of Exploding/Vanishing Gradient"
date: 2018-11-01
categories: [Machine Learning]
tags: [ml, rnn]
math: true
---

This article is my brief review of **Recurrent Neural Network (RNN)**. It introduces the RNN model, and two associated problems: **exploding gradient** and **vanishing gradient**.

# Recurrent Neural Network (RNN)

RNN is a type of neural network designed to deal with time series, or sequence modeling. Since there are a lot of good online materials about it, I won't be reviewing the RNN model itself. For a general introduction, I found the Stanford NLP course is helpful: [Lecture 8 Recurrent Neural Network and Language Models](https://www.youtube.com/watch?v=Keqep_PKrY8).

Simply said, RNN uses the same set of weights $W, U$ to compute a hidden state $h_t$ (or $s_t$) at each time step $t$; at next step $t+1$, the input for $h_{t+1}$ is the last hidden state $h_t$, in conjunction with new time series data $x_{t+1}$. Output and loss are computed at each time step using weights $V$.

### Motivation

One of the most common use cases of RNN is to build the language model. Because it is not practical to learn a joint probability distribution for all language sequences, a general language model approximates the probability by assuming Markov chain, which means the probability of a sequence is the multiplication of conditional probabilities, as shown below.

![lm](/assets/img/legacy/lm.png)

The usage of RNN allows us to softly condition on all previous input $x_1, ..., x_{t-1}$ at time step $t$. The hidden state $h_t$ contains all information from $x_1$ to $x_t$ (in theory). RNN is essentially a low dimensional matrix representation of the probability model; since it doesn't have too many parameters to learn, it makes the training process practical.

However, the basic RNN model also has practical issues during training process. For the rest of the article, we will focus on these problems and how to address them.

### Backpropagation for RNN

To introduce the exploding and vanishing gradient problems, we need to first go through the backprop process for RNN.

There are three weight parameters in RNN: $W$ for hidden state $h$, $U$ for input $x$, and $V$ for output $o$. Let's examine $V$ first, which turns out to be the easiest case.

Notate the loss as $E_t$ at time step $t$. The loss is a function of $o_t$, and $o_t$ is $Vh_t$ with nonlinearity: $o_t =\Phi (y_t) =\Phi (Vh_t)$. Take the partial derivative of $E_t$ w.r.t $V$:

$$
\begin{align}
\dfrac{\partial E_t}{\partial V} &= \dfrac{\partial E_t}{\partial o_t} \dfrac{\partial o_t}{\partial V}\\
&= \dfrac{\partial E_t}{\partial o_t} \dfrac{\partial o_t}{\partial y_t} \dfrac{\partial y_t}{\partial V}\\
&= \dfrac{\partial E_t}{\partial o_t} \dfrac{\partial o_t}{\partial y_t} h_t\\
\end{align}
$$

Here, $\dfrac{\partial E_t}{\partial o_t}$ and $\dfrac{\partial o_t}{\partial y_t}$ depends on the specific loss and nonlinearity function being used. The most important thing to notice is, at each time step $t$, $\dfrac{\partial E_t}{\partial V}$ only requires current hidden state $h_t$ ($h_t$ is not a function of $V$). This has no much difference with FeedForward Neural Network. (Note that the objective doesn't necessarily need to be $V$ which is FFNN; here we assume $V$ for simplicity.)

Next, let's look at $W$ (also, same for $U$). Take the partial derivative of $E_t$ w.r.t $W$:

$$
\begin{align}
\dfrac{\partial E_t}{\partial W} &= \dfrac{\partial E_t}{\partial o_t} \dfrac{\partial o_t}{\partial W}\\
&= \dfrac{\partial E_t}{\partial o_t} \dfrac{\partial o_t}{\partial h_t} \dfrac{\partial h_t}{\partial W}\\
\end{align}
$$

$\dfrac{\partial E_t}{\partial o_t}$ and $\dfrac{\partial o_t}{\partial h_t}$ depends on the specific loss function used; we can safely put them aside (again, note that in other cases, $\dfrac{\partial o_t}{\partial h_t}$ doesn't necessarily need to be $V$; it depends on the network structure of the objective). The key thing is to determine $\dfrac{\partial h_t}{\partial W}$.

$h_t = \Phi (Wh_{t-1} + Ux_t)$. Notice that $h_{t-1}$ is also a function of $W$, which in turns depends on $h_{t-2}, ..., h_0$, and all of these hidden states are functions of $W$. Therefore, using multivariate chain rule, we can get the following form:

$$
\begin{align}
\dfrac{\partial E_t}{\partial W} &= \dfrac{\partial E_t}{\partial o_t} \dfrac{\partial o_t}{\partial h_t} \dfrac{\partial h_t}{\partial W}\\
&= \dfrac{\partial E_t}{\partial o_t} \dfrac{\partial o_t}{\partial h_t} \sum_{k=0}^{t} (\dfrac{\partial h_t}{\partial h_k} \dfrac{\partial h_k}{\partial W})
\end{align}
$$

We can then use chain rule on $\dfrac{\partial h_t}{\partial h_k}$ as well.

$$
\begin{align}
\dfrac{\partial E_t}{\partial W} &= \dfrac{\partial E_t}{\partial o_t} \dfrac{\partial o_t}{\partial h_t} \sum_{k=0}^{t} (\dfrac{\partial h_t}{\partial h_k} \dfrac{\partial h_k}{\partial W})\\
&= \dfrac{\partial E_t}{\partial o_t} \dfrac{\partial o_t}{\partial h_t} \sum_{k=0}^{t} ((\prod_{j=k+1}^{t} \dfrac{\partial h_j}{\partial h_{j-1}}) \dfrac{\partial h_k}{\partial W})\\
&= \dfrac{\partial E_t}{\partial o_t} \dfrac{\partial o_t}{\partial h_t} \sum_{k=0}^{t} ((\prod_{j=k+1}^{t} W \cdot f(h_{j-1})) \dfrac{\partial h_k}{\partial W})
\end{align}
$$

where $f$ is just the derivative of nonlinearity upon hidden state.

### Encoder

RNN is often referred as a type of "encoder". We can think of RNN using $U$ and $W$ to encode the sequence information to a fixed-dimension vector representation. The objective of the encoding process can be flexible. For example, it can be a simple matrix $V$ (or equivalently, FFNN) for some classification task, or the network can be jointly learned with "decoder" on some other downstream task (usually seen in sequence-to-sequence model).

## Exploding Gradient

We can see from above that, at each time step, the backprop needs to compute $(W \cdot f)^{t-k}$ for all previous step $k$. If there is no nonlinearity, $\vert W^{t-k} \vert$ would quickly blow up if $\vert W \vert > 1$, which would make the program crash. This is called exploding gradient problem, and it basically makes the training unpractical.

However, it turns out that exploding gradient is not that problematic, because:

1. The problem is easy to notice and diagnose, because the derivative would become Nan very quickly and crash the program.
2. There are some easy hacks that can effectively prevent exploding gradient. One of them is called gradient clipping, which simply throttles the scale of gradient during backprop, and it turns out to work well. For more details, please refer to this paper.

## Vanishing Gradient

Same as exploding gradient, vanishing gradient is largely caused by $W^{t-k}$. If $\vert W \vert < 1$, then $\vert W^{t-k} \vert$ would quickly shrink to $0$, especially with sigmoid nonlinearity. The gradient will become $0$ for previous long-distance steps, which makes RNN hard to learn long-range dependencies.

Vanishing gradient is more problematic than exploding gradient, because it is a general problem not only to RNN, but also to any deep neural network with many layers. Because the derivative of previous layers depends on that of later layers, it is hard to learn previous layers if later layers have small derivative. This is a main reason that we prefer ReLu as activation rather than sigmoid. But even with Relu, the vanishing gradient problem still exists.

## Long Short-Term memory (LSTM)

LSTM is a variant of RNN, which addresses the exploding and vanishing gradient problems. I might review it in another post.

