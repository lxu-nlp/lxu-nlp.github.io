---
layout: post
title: "Machine Learning (6) Boosting: AdaBoost and Forward Stagewise Additive Modeling"
date: 2018-10-29
categories: [Machine Learning]
tags: [ml, boosting]
math: true
---

This article is part of my review of Machine Learning course. It introduces **Forward Stagewise Additive Modeling** and **AdaBoost**, and gives the proof that AdaBoost is actually a special case of Forward Stagewise Additive Modeling when the loss function is exponential loss.

In this article, we assume the problem context is binary classification. Some pictures are from the textbook "The Elements of Statistical Learning" (ESL).

## Boosting

Let's review the concept of Boosting briefly: it is a procedure that combines many "weak" learners to produce a "strong" output. Here the "weak" learner means the output of it is better than random guessing, and Boosting combines them together and can drastically lower the error rate.

### Forward Stagewise Additive Modeling (Forward Stagewise Boosting)

We introduce the general form of Additive Model: $$f(x) = \sum_1^M \beta_m b(x; \gamma_m)$$

Here, $b(x; \gamma_m)$ is the m-th weak learner/classifier which gives output $+1, -1$. $\beta_m$ is the coefficient/weight associated with each weak learner. The final output is a weighted summation through all learners.

Let's look at the algorithm to fit the additive model, called Forward Stagewise Additive Modeling, or Forward Stagewise Boosting:

![boost1](/assets/img/legacy/boosting-1.png)

##### Observation

During each iteration, $\beta_m$ and $\gamma_m$ are fitted such that current model $f_m(x)$ achieves minimum loss on the training set. The loss is computed directly on the performance of $f_{m-1}(x) +\beta_m b(x; \gamma_m) $; and parameters $\beta_m$ and $\gamma_m$ are fitted directly towards this loss.

The algorithm is very intuitive. Each time it optimizes the current model towards minimum loss, and select $\beta_m$ and $\gamma_m$ based on this criterion. We will come back to this algorithm after introducing AdaBoost.

### Adaboost

AdaBoost is one of the famous boosting algorithms. It can dramatically increase the performance of even a very weak classifier. "Most of the work has centered on using classification trees as the base learner, where improvements are often most dramatic. (from ESL)" AdaBoost with trees is also referred to as the “best off-the-shelf classifier in the world”.

Let's look at the AdaBoost algorithm for binary classification.

![boost1](/assets/img/legacy/boosting-2.png)

##### Observation

During each iteration, the algorithm trains the m-th weak learner on a weighted dataset, where weights $w_i$ come from the last iteration. After training and getting an error term $err_m$, it assigns a coefficient $\alpha_m$ to the m-th weak learner based on $err_m$. Lastly, it re-computes the dataset weights $w_i$ based on $\alpha_m$ (or $err_m$, since $\alpha_m$ is computed from $err_m$).

Overall, AdaBoost will assign larger weights to the samples that didn't get classified correctly during the previous iterations, so that the next learner can focus more on the hard-to-learn samples. The weight will keep increasing until the sample is classified correctly.

## Relation between AdaBoost and Forward Stagewise Additive Modeling

From the algorithms, AdaBoost and Forward Stagewise Additive Modeling are not exactly the same, and have different ways to handle hard-to-learn instances. However, these two are essentially the same algorithm!

Now let's prove that AdaBoost is equivalent to Forward Stagewise Additive Modeling when using exponential loss.

From 2(a) of the Forward Stagewise Additive Modeling, the parameters to calculate are:

$$(\beta_m, \gamma_m) = argmin \sum L(y_i, f_{m-1}(x_i) + \beta b(x_i; \gamma))$$

Here $b(x_i, \gamma)$ is classifier $G(x_i)$ that gives output $+1, -1$. Use exponential loss:  $L(y, f(x)) = \exp (-yf(x))$. We have:

$$
\begin{align}
(\beta_m, G_m) &= argmin \sum \exp [-y_i(f_{m-1}(x_i) + \beta G(x_i))]\\
&= argmin \sum w_i^{(m)} \exp (-\beta y_i G(x_i)) \quad with \; w_i^{(m)} = \exp (-y_i f_{m-1}(x_i))\\
\end{align}
$$

From this step, we can see regardless the value of $\beta$, $G_m$ must follow the below property to achieve minimum loss: $G_m$ should minimize the weighted error rate. Otherwise, the loss could always be lower for the same $\beta$.

$$G_m = argmin \sum w_i^{(m)} \mathbb{1}(y_i \ne G(x_i))$$

Therefore we have the solution for $G_m$, and now we only need to tackle the $\beta_m$:

$$
\begin{align}
(\beta_m) &= argmin \sum w_i^{(m)} \exp (-\beta y_i G_m(x_i))\\
&= argmin\; e^{-\beta} \cdot \sum_{y_i = G_m(x_i)} w_i^{(m)} + e^{\beta} \cdot \sum_{y_i \ne G_m(x_i)} w_i^{(m)}\\
&= argmin\; (e^{\beta} - e^{-\beta}) \cdot \sum w_i^{(m)} \mathbb{1} (y_i \ne G_m(x_i)) + e^{-\beta} \cdot \sum w_i^{(m)}\\
\end{align}
$$

To get the $\beta$ that minimizes the above formula, take the derivative of it on $\beta$ and set it to $0$:

$$
(e^{\beta} + e^{-\beta}) \cdot \sum w_i^{(m)} \mathbb{1} (y_i \ne G_m(x_i)) - e^{-\beta} \cdot \sum w_i^{(m)} = 0\\
(e^{2\beta} + 1) \cdot \sum w_i^{(m)} \mathbb{1} (y_i \ne G_m(x_i)) = \sum w_i^{(m)}\\
e^{2\beta} = \dfrac{\sum w_i^{(m)}}{\sum w_i^{(m)} \mathbb{1} (y_i \ne G_m(x_i))} - 1\\
$$

Let $err_m = \dfrac{\sum w_i^{(m)} \mathbb{1} (y_i \ne G_m(x_i))}{\sum w_i^{(m)}}$ (same weighted error rate in AdaBoost), we have:
$$
\begin{align}
e^{2\beta} &= \dfrac{1}{err_m} - 1 = \dfrac{1 - err_m}{err_m}\\
\beta &= \dfrac{1}{2} \log \dfrac{1 - err_m}{err_m}
\end{align}
$$

Therefore, we have solution for $\beta_m$, which is the coefficient for current learner $G_m$. Notice that in AdaBoost, $\alpha_m$ is of exactly this form; more concretely, $\alpha_m = 2 \beta_m$. The only difference is a constant 2.

Now that we see the coefficient for current learner $G_m$ is the same for both AdaBoost and Forward Stagewise Additive Modeling, we get to the weight $w_i$ update. The current model is updated:

$$f_m(x) = f_{m-1}(x) + \beta_m G_m(x)$$

Therefore, the weights for the next iteration are:

$$w_i^{(m+1)} = w_i^{(m)} \cdot \exp [-\beta_m y_i G_m(x_i)]$$

Remember $\alpha_m = 2 \beta_m$, we have:

$$w_i^{(m+1)} = w_i^{(m)} \cdot \exp[\alpha_m \mathbb{1}(y_i \ne G_m(x_i)] \cdot \exp[-\beta_m]$$

We can disregard the extra term $\exp[-\beta_m]$ because it applies to all samples regardless of $i$. Notice that this is the same weight update method in AdaBoost.

Now the proof is done. We see that coefficient and weight update from AdaBoost can be derived from Forward Stagewise Boosting using exponential loss function.
