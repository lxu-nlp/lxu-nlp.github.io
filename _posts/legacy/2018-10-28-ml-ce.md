---
layout: post
title: "Machine Learning (5) Classification: Brief Review of Cross Entropy Loss"
date: 2018-10-28
categories: [Machine Learning]
tags: [ml]
math: true
---

This article is a brief review of common loss functions for the classification problems; specifically, it discusses the Cross-Entropy function for multi-class and binary classification loss.

Cross-entropy loss is fundamental in most classification problems, therefore it is necessary to make sense of it. Since there are already lots of articles talking about the details, this article is more like a high-level review.

## Cross-Entropy

**Entropy** is a measure of information, and is defined as follows: let $x$ be a random variable, $p(x)$ be its probability function, the entropy of $x$ is:

$$E(x) = - \sum_x p(x) \log p(x)$$

Entropy comes from the information theory, and represents the minimum number of bits to encode the information of $x$. The more random the $x$ is, the larger the entropy. To the extreme case where $x$ is a constant (not random at all), the entropy is $- 1 \cdot \log 1 = 0$.

**Cross-entropy** is what we usually use for the classification loss, and it has the following form: for any sample, let $t$ be the ground truth of its class distribution, and $s$ be its estimated class distribution; $t_i$, $s_i$ means the true and estimated probability at class $i$ respectively. The cross-entropy is:

$$CE = - \sum_i t_i \log s_i$$

$s$ is usually a score after some activation function; specially, softmax activation for categorical classification, or sigmoid activation for binary classification. Details will be discussed in the rest of this article.

The more similarity between $t$ and $s$, the lower the cross-entropy. By setting the cross-entropy as training objective, we are telling the model to learn the ground truth.

## Categorical/Softmax Cross-Entropy Loss

Traditionally, categorical CE is used when we want to classify each sample to one single class, out of many candidate classes. Technically it can also be used to do multi-label classification, but it is tricky to assign the ground truth probabilities among the positive classes, so for simplicity, we here assume the single-label case.

For the single-label classification, the ground truth vector $t$ is one-hot: $1$ for the positive class, $0$ for all the other negative classes. The activation function is Softmax:

$$f(s)_i = \dfrac{e^{s_i}}{\sum_j e^{s^j}}$$

where $s$ is the predicted score vector, and $f(s)$ can be seen as the probability distribution vector. The cross-entropy loss now has the following form:

$$CE = - \sum_i t_i \log f(s)_i$$

which can be rewrite as the following form since $t$ is one-hot:

$$CE = - \log \dfrac{e^{s_p}}{\sum_j e^{s_j}}$$

where $s_p$ means the predicted score for the positive class. This means in the categorical CE, we only care about the predicted probability of the positive class, and the cross-entropy doesn't care how the predicted probability gets distributed for the other negative classes. However, in the softmax, the probability of each class is **not independent**: for each $f(s)_i$, the denominator takes account in all other classes $f(s)_j$. Therefore, as long as we push the probability of the true positive class towards $1$, all other negative classes will automatically have probabilities $0$.

### Derivative

From the above categorical cross-entropy, we can easily calculate the partial derivative towards $s_i$. We can break it up into two cases: $i = p$ (positive class) and $i = n$ (negative classes).

$$CE = - \log \dfrac{e^{s_p}}{\sum_j e^{s_j}}$$

$i = p$:
$$\dfrac{\partial CE}{\partial s_p} = - (\dfrac{\sum_j e^{s_j}}{e^{s_p}} \cdot\dfrac{\partial\dfrac{e^{s_p}}{\sum_j e^{s_j}}}{\partial s_p})$$

Using the derivative rule $f(x) = \dfrac{g(x)}{h(x)}$, $f'(x) = \dfrac{g'(x)h(x) - h'(x)g(x)}{h^2(x)}$, we have

$$\dfrac{\partial CE}{\partial s_p} = - (\dfrac{\sum_j e^{s_j}}{e^{s_p}} \cdot \dfrac{e^{s_p} \sum_j e^{s_j} - e^{s_p} e^{s_p}}{(\sum_j e^{s_j})^2})\\
= -(\dfrac{\sum_j e^{s_j} - e^{s_p}}{\sum_j e^{s_j}})\\
= -(1 - \dfrac{e^{s_p}}{\sum_j e^{s_j}}) = f(s)_p - 1$$

$i = n$:

$$\dfrac{\partial CE}{\partial s_n} = - (\dfrac{\sum_j e^{s_j}}{e^{s_p}} \cdot\dfrac{\partial\dfrac{e^{s_p}}{\sum_j e^{s_j}}}{\partial s_n})\\
=- (\dfrac{\sum_j e^{s_j}}{e^{s_p}} \cdot \dfrac{-e^{s_p}}{(\sum_j e^{s_j})^2} \cdot e^{s_n})\\
= \dfrac{e^{s_n}}{\sum_j e^{s_j}} = f(s)_n$$

In summary,

$$
\dfrac{\partial CE}{\partial s_i} =
\begin{cases}
f(s)_i - 1, & \text{if i is positive class}\\
f(s)_i, & \text{if i is negative class}
\end{cases}
$$

## Binary/Sigmoid Cross-Entropy Loss

Binary cross-entropy loss is used when each sample could belong to many classes, and we want to classify into each class independently; for each class, we apply the sigmoid activation on its predicted score to get the probability. This is the biggest difference from the softmax CE: in Sigmoid CE, classes are independent, where in softmax CE, classes are dependent.

Sigmoid function:

$$\sigma (x) = \dfrac{1}{1 + e^{-x}}$$

where $\sigma$ can squash any real number into $(0, 1)$ interpreted as probability.

$$CE = - \sum_{i = \{p, n\}} t_i \log (\sigma(s_i))\\
= -t_p \log (\sigma(s_p)) \; - (1 - t_p) \log (1 -\sigma(s_p))$$

Note that $t_p$ is either $1$ or $0$. The above binary cross-entropy is also called negative log-likelihood. It only involves positive class since the negative class is just its complement.

If we substitute $\sigma$, it looks like this:

$$CE = t_p \log (1 + e^{-s_p}) + (1 - t_p) \log (1 + e^{s_p})$$

### Derivative

The derivative for binary CE is as follows (derivation is skipped):

$$\dfrac{\partial CE}{\partial s_i} = t_p (\sigma (s_p)\; - 1) + (1 - t_p) \sigma(s_p)$$
